{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models, GPT3, LangChain and Chatbots\n",
        "\n",
        "## Martin Callaghan\n",
        "* Lecturer in CS, University of Leeds\n",
        "* sometimes-RSE\n",
        "\n",
        "Large language models (LLMs) are a transformative technology, enabling developers to build applications that they previously could not. But using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you are able to combine them with other sources of computation or knowledge.\n",
        "\n",
        "LangChain is a very new Python package that allows us to interact with LLMs and develop applications like Chatbots! This Notebook is based on some of the examples in the Langchain documentation.\n",
        "\n",
        "See here for more information: https://langchain.readthedocs.io/en/latest/?"
      ],
      "metadata": {
        "id": "EO_zcqFwh95B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install some helper packages\n",
        "\n",
        "You'll need to install Langchain (a Python package to build prompts and LLM toolchains, plus the OpenAI package to make using their API easier."
      ],
      "metadata": {
        "id": "i5B8SH99in0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZatDRDzLwbz"
      },
      "outputs": [],
      "source": [
        "# LangChain is the Python package that allws us to build prompts and language chains\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll be using the OpenAI LLMs (including GPT3) so we need their API\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "hu_neX0jL09f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install the SERPAPI google results scraping package\n",
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "N910vGvadl1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register with Open AI\n",
        "\n",
        "1. To use the Open AI API, you'll need to register with them. The API is **not** free but this Notebook shouldn't take more than 50p to run.\n",
        "\n",
        "Register here: https://openai.com/blog/openai-api\n",
        "\n",
        "2. You'll also need access to a Google search API via SERPAPI.\n",
        "\n",
        "Register here: https://serpapi.com/\n"
      ],
      "metadata": {
        "id": "i5Iap3D5cZR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to set up and use an API key to allow access to GPT-3 via an API\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<put-yout-API-key-here>\""
      ],
      "metadata": {
        "id": "vOMpULQqMEI5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We also need access to a Google search API for a later exercise\n",
        "os.environ[\"SERPAPI_API_KEY\"] =\"<put-yout-API-key-here>\""
      ],
      "metadata": {
        "id": "HtjkvGesjTSi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using OpenAI's LLMs. There are many others we can use, such as the public (free) ones published by HuggingSpace or BLOOM.\n",
        "\n",
        "The particular variant of GPT-3 provided through the default API is called InstructGPT.  \n",
        "\n",
        "https://openai.com/blog/instruction-following/\n",
        "\n",
        "It produces output that is better at following user intentions and is more truthful and less toxic.\n",
        "\n",
        "Remember our principles of Responsible AI!\n",
        "\n",
        "eg. from Microsoft: https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl\n"
      ],
      "metadata": {
        "id": "dxNjEwNOuO4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting some predictions from the LLM\n",
        "\n",
        "The most basic building block of LangChain is calling an LLM on some input. Let’s look at a simple example of how to do this. \n",
        "\n",
        "For this example, let's say we are tryong to create an application that generates short and snappy names for our reserch projects.\n",
        "\n",
        "In order to do this, we first need to import the LLM wrapper."
      ],
      "metadata": {
        "id": "jidyEX0YMoGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "1k8JEvvfMd2P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and then set the \"temperature\" of the results. Temperature is an option passed into the LLM. A temperature of 0 means the response is deterministic, in that it always returns the same result. A temperature of greater than zero results in increasing randomness in the answer.  \n",
        "\n",
        "Here we set temperature to be 0.9 to give a random output (for OpenAI's LLMs the temperature should be between 0 and 1).\n",
        "\n",
        "You can read more about temperature [here](https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be)."
      ],
      "metadata": {
        "id": "vuBR5yX6eJpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some random outputs\n",
        "llm = OpenAI (temperature = 0.9)"
      ],
      "metadata": {
        "id": "fACA4vgDluMi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What would be a good name for a research project that creates chatbots for student education\""
      ],
      "metadata": {
        "id": "GWQFg_l61Dck"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (llm(text))"
      ],
      "metadata": {
        "id": "p-riBLio1MHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try running the cell above a few times and you'll get different answers."
      ],
      "metadata": {
        "id": "IDhyNZ4Jhr8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Managing prompts for LLMs\n",
        "\n",
        "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you are probably taking user input and constructing a prompt, and then sending that to the LLM."
      ],
      "metadata": {
        "id": "bo1L0Pr7OFah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a template for a prompt\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate (\n",
        "    input_variables= [\"project_name\"],\n",
        "    template = \"What would be a good name for a research project that investigates {project_name}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "7eX4YxGzlvB5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (prompt.format (project_name = \"Chatbot-Enhanced Student Education\"))"
      ],
      "metadata": {
        "id": "BuBI_8ai2rKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Combining LLMs and prompts into Chains\n",
        "\n",
        "A real application is not just one primitive (such as a PromptTemplate or a simple prediction), but rather a combination of them.\n",
        "\n",
        "A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.\n",
        "\n",
        "The most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM."
      ],
      "metadata": {
        "id": "Hubn8tpgPJkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extending the previous example, we can construct an LLMChain which takes user input, \n",
        "# formats it with a PromptTemplate, and then passes the formatted response to an LLM.\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"project_name\"],\n",
        "    template=\"What would be a good name for a research project that investigates {project_name}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "33JR1Pw-l2pW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:"
      ],
      "metadata": {
        "id": "p8s5MF-rP-68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "R7X-TD_ul5b_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run (\"Chatbot-Enhanced Student Education\")"
      ],
      "metadata": {
        "id": "S1vj_kzP4CKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Calling chains dynamically based on user input\n",
        "\n",
        "So far the chains we’ve looked at run in a predetermined order.\n",
        "\n",
        "Agents no longer do: they use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n",
        "\n",
        "Some concepts:\n",
        "\n",
        "* **Tool**: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n",
        "* **LLM**: The language model powering the agent.\n",
        "* **Agent**: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. \n",
        "\n",
        "**Agents**: https://langchain.readthedocs.io/en/latest/modules/agents/agents.html\n",
        "\n",
        "**Tools**: https://langchain.readthedocs.io/en/latest/modules/agents/tools.html"
      ],
      "metadata": {
        "id": "aOEx9nczXnaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "1eUZGj1ql9wV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's load the language model we're going to use to control the agent.\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "zxIFHdrL5MJT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "T6x4ngNK5e8r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "-JY1ZNw45oCe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask it a question\n",
        "agent.run(\"Who is Brooklyn Beckham's wife? What is her current age raised to the 0.23 power?\")"
      ],
      "metadata": {
        "id": "-P9fogdJ5vkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try another one:\n",
        "agent.run(\" Who was the lead singer of 'The Animals'? How did he die?\")"
      ],
      "metadata": {
        "id": "YFOJFJhdiAGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You  might want to see if this is true!\n",
        "\n",
        "https://en.wikipedia.org/wiki/Eric_Burdon\n",
        "\n",
        "How do you think this happened?  What is the reasoning here?"
      ],
      "metadata": {
        "id": "BJJSKnk_iqQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Adding state or memory to our chains\n",
        "\n",
        "Often, you may want a chain or agent to have some concept of “memory” so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of “short-term memory”.\n",
        "\n",
        "LangChain provides several specially created chains just for this purpose. This notebook walks through using one of those chains (the ConversationChain) with two different types of memory."
      ],
      "metadata": {
        "id": "S7AT2LkrZWoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ],
      "metadata": {
        "id": "7kqXy2LSmB1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
      ],
      "metadata": {
        "id": "EA6J_MfH7Kgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replicating ChatGPT (sort of)\n",
        "\n",
        "A simple chain that replicates ChatGPT by combining (1) a specific prompt, and (2) the concept of memory."
      ],
      "metadata": {
        "id": "2phXLSNBZycG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
        "from langchain.chains.conversation.memory import ConversationalBufferWindowMemory\n",
        "\n",
        "\n",
        "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
        "Assistant is designed to be able to assist with a wide range of tasks, from \n",
        "answering simple questions to providing in-depth explanations and discussions \n",
        "on a wide range of topics. \n",
        "\n",
        "As a language model, Assistant is able to generate human-like text based on the \n",
        "input it receives, allowing it to engage in natural-sounding conversations and \n",
        "provide responses that are coherent and relevant to the topic at hand.\n",
        "\n",
        "Assistant is constantly learning and improving, and its capabilities are \n",
        "constantly evolving. It is able to process and understand large amounts of text, \n",
        "and can use this knowledge to provide accurate and informative responses to a \n",
        "wide range of questions. Additionally, Assistant is able to generate its own \n",
        "text based on the input it receives, allowing it to engage in discussions and \n",
        "provide explanations and descriptions on a wide range of topics.\n",
        "\n",
        "Overall, Assistant is a powerful tool that can help with a wide range of tasks \n",
        "and provide valuable insights and information on a wide range of topics. \n",
        "Whether you need help with a specific question or just want to have a \n",
        "conversation about a particular topic, Assistant is here to assist.\n",
        "\n",
        "{history}\n",
        "Human: {human_input}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"human_input\"], \n",
        "    template=template\n",
        ")\n",
        "\n",
        "\n",
        "chatgpt_chain = LLMChain(\n",
        "    llm=OpenAI(temperature=0), \n",
        "    prompt=prompt, \n",
        "    verbose=True, \n",
        "    memory=ConversationalBufferWindowMemory(k=2),\n",
        ")"
      ],
      "metadata": {
        "id": "yCHy11AaZpvu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chatgpt_chain.predict(human_input= \"\"\"I want you to act as a Linux \n",
        "terminal. I will type commands and you will reply with what the terminal should \n",
        "show. I want you to only reply wiht the terminal output inside one unique code \n",
        "block, and nothing else. Do not write explanations. Do not type commands unless \n",
        "I instruct you to do so. When I need to tell you something in English I will \n",
        "do so by putting text inside curly brackets {like this}. My first command is pwd.\"\"\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "uwRbKN8qar71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chatgpt_chain.predict(human_input=\"ls ~\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "8iauO1qScJuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chatgpt_chain.predict(human_input=\"cd ~\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "K6a3iK32-Zir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chatgpt_chain.predict(human_input=\"{Please make a file jokes.txt inside and put some jokes inside}\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "FYXFkdth-jGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chatgpt_chain.predict(human_input=\"\"\"echo -e \"x=lambda y:y*5+3;print('Result:' + str(x(6)))\" > run.py && python3 run.py\"\"\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "rifSozDe_Bmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This example opens up a whole new area of work called **Prompt Engineering**\n",
        "(basically how to ask an LLM questions)\n",
        "\n",
        "There's a good Medium article all about that [here](https://medium.com/mlearning-ai/from-zero-shot-to-chain-of-thought-prompt-engineering-choosing-the-right-prompt-types-88800f242137 )\n",
        "\n"
      ],
      "metadata": {
        "id": "03gbxajGkeHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More applications using LangChain\n",
        "\n",
        "https://langchain.readthedocs.io/en/latest/gallery.html"
      ],
      "metadata": {
        "id": "pc0qAFclmNAb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kxWYQuXYmUtm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}